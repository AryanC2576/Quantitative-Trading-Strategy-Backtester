{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1843854,"sourceType":"datasetVersion","datasetId":1092483}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:13.915715Z","iopub.execute_input":"2025-08-10T06:57:13.916148Z","iopub.status.idle":"2025-08-10T06:57:13.932246Z","shell.execute_reply.started":"2025-08-10T06:57:13.916118Z","shell.execute_reply":"2025-08-10T06:57:13.931129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCan we build a profitable, statistically robust trading strategy on a single stock using a blend of classic indicators and a simple machine learning model?\n\nIn the world of quantitative finance, a single backtest result can be misleading. It's easy to create a strategy that looks great on paper but fails in the real world due to lookahead bias or overfitting. The goal of this notebook is to move beyond a simple backtest and implement a professional-grade research framework to rigorously test a quantitative trading strategy.\nOur objective is to build and validate a strategy for Oracle (ORCL) stock from 1995 to 2014. We will combine multiple alpha sources—Momentum, RSI, MACD, and a Ridge Regression model—into a single, blended signal.\n\nMost importantly, we will use a Walk-Forward Backtesting methodology. Instead of training our model on the entire dataset at once (which \"cheats\" by using future information), we will simulate real-world performance by training the model on a rolling window of past data and making predictions on a subsequent, unseen window. This is the core of our robust approach.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:13.933795Z","iopub.execute_input":"2025-08-10T06:57:13.934247Z","iopub.status.idle":"2025-08-10T06:57:13.965363Z","shell.execute_reply.started":"2025-08-10T06:57:13.934220Z","shell.execute_reply":"2025-08-10T06:57:13.963821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install ta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:13.969491Z","iopub.execute_input":"2025-08-10T06:57:13.970885Z","iopub.status.idle":"2025-08-10T06:57:17.975053Z","shell.execute_reply.started":"2025-08-10T06:57:13.970850Z","shell.execute_reply":"2025-08-10T06:57:17.973765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ta\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:17.976320Z","iopub.execute_input":"2025-08-10T06:57:17.976702Z","iopub.status.idle":"2025-08-10T06:57:17.983590Z","shell.execute_reply.started":"2025-08-10T06:57:17.976668Z","shell.execute_reply":"2025-08-10T06:57:17.982477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nTable of Contents\nThe Framework: Object-Oriented Design\n\nStep 1: Loading and Cleaning the Data\n\nStep 2: Engineering the Features\n\nStep 3: The Walk-Forward Backtesting Engine\n\nStep 4: Analyzing the Performance\n\nRolling Period Metrics\n\nFinal Aggregated Metrics\n\nInterpreting the Visual Dashboard\n\nConclusion & Future Work\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:17.986055Z","iopub.execute_input":"2025-08-10T06:57:17.986347Z","iopub.status.idle":"2025-08-10T06:57:18.014936Z","shell.execute_reply.started":"2025-08-10T06:57:17.986329Z","shell.execute_reply":"2025-08-10T06:57:18.013733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n1. The Framework: Object-Oriented Design\nTo ensure our code is clean, modular, and reusable, we've structured this entire project using four Python classes, each with a single responsibility:\n\nDataManager: Handles loading and initial data cleaning.\n\nFeatureEngineer: Creates all our predictive features (alpha signals).\n\nWalkForwardBacktester: The core engine that runs the robust backtest.\n\nPerformanceAnalyzer: Calculates all metrics and generates our final plots.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.015959Z","iopub.execute_input":"2025-08-10T06:57:18.016261Z","iopub.status.idle":"2025-08-10T06:57:18.063603Z","shell.execute_reply.started":"2025-08-10T06:57:18.016212Z","shell.execute_reply":"2025-08-10T06:57:18.054483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n2. Step 1: Loading and Cleaning the Data\nOur first step is to load the historical price data for Oracle (ORCL) from the provided text file. \nWe'll use pandas to parse the data, set the 'Date' column as our index, and perform initial cleaning by handling any infinite values and dropping rows with missing price data.\nThis ensures we have a clean, reliable dataset to work with.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.064879Z","iopub.execute_input":"2025-08-10T06:57:18.065189Z","iopub.status.idle":"2025-08-10T06:57:18.099199Z","shell.execute_reply.started":"2025-08-10T06:57:18.065165Z","shell.execute_reply":"2025-08-10T06:57:18.097831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Data Fetching\nclass DataFetch:\n    #Handles loading, cleaning, and initial preprocessing of financial data.\n    def __init__(self, file_path):\n        self.file_path = file_path\n        self.df = None\n\n    def load_data(self):\n        print(\"1. Loading and cleaning data...\")\n        self.df = pd.read_csv(self.file_path, parse_dates=['Date'], index_col='Date')\n        self.df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        # Ensure all necessary columns are present\n        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n        self.df.dropna(subset=required_cols, inplace=True)\n        print(f\"Data loaded successfully. Shape: {self.df.shape}\")\n        return self.df.copy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.100571Z","iopub.execute_input":"2025-08-10T06:57:18.100849Z","iopub.status.idle":"2025-08-10T06:57:18.124327Z","shell.execute_reply.started":"2025-08-10T06:57:18.100827Z","shell.execute_reply":"2025-08-10T06:57:18.122834Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n3. Step 2: Engineering the Features\nA strategy is only as good as its predictive signals (features). Here, we engineer a comprehensive set of features that we hypothesize will have some power to predict future returns. This includes:\n\nClassic Indicators: Momentum, Volatility, RSI, and MACD.\n\nAdvanced Indicators: We also add the ADX, Aroon, Stochastic Oscillator, and the volume-based Chaikin Money Flow (CMF) to provide a richer set of inputs for our model.\n\nRegime-Aware Signal: We create a dynamic RSI_Signal that adjusts its buy/sell thresholds based on the market's volatility regime. In high-volatility periods, it requires a stronger signal to trade, and in low-volatility periods, it is more sensitive.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.125830Z","iopub.execute_input":"2025-08-10T06:57:18.126185Z","iopub.status.idle":"2025-08-10T06:57:18.152531Z","shell.execute_reply.started":"2025-08-10T06:57:18.126157Z","shell.execute_reply":"2025-08-10T06:57:18.150788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Feature Engineering\nclass FeatureEngineer:\n    def __init__(self, df):\n        self.df = df.copy()\n\n    def create_base_features(self):\n        print(\"2. Engineering base features (Momentum, Volatility, RSI, MACD, ADX, etc.)...\")\n        self.df['Returns'] = self.df['Close'].pct_change()\n        self.df['Momentum'] = self.df['Close'] / self.df['Close'].shift(21) - 1\n        self.df['Volatility'] = self.df['Returns'].rolling(window=21).std() * np.sqrt(252)\n        self.df['RSI'] = ta.momentum.RSIIndicator(self.df['Close'], window=14).rsi()\n        \n        macd = ta.trend.MACD(self.df['Close'])\n        self.df['MACD_diff'] = macd.macd_diff()\n\n        # ADX (Average Directional Movement Index)\n        adx_indicator = ta.trend.ADXIndicator(high=self.df['High'], low=self.df['Low'], close=self.df['Close'], window=14)\n        self.df['ADX'] = adx_indicator.adx()\n\n        # Aroon Indicator\n        aroon_indicator = ta.trend.AroonIndicator(high=self.df['High'], low=self.df['Low'], window=25)\n        self.df['Aroon_Up'] = aroon_indicator.aroon_up()\n        self.df['Aroon_Down'] = aroon_indicator.aroon_down()\n\n        # Stochastic Oscillator\n        stoch_indicator = ta.momentum.StochasticOscillator(high=self.df['High'], low=self.df['Low'], close=self.df['Close'], window=14, smooth_window=3)\n        self.df['Stoch_K'] = stoch_indicator.stoch()\n        \n        # Chaikin Money Flow (CMF)\n        self.df['CMF'] = ta.volume.ChaikinMoneyFlowIndicator(high=self.df['High'], low=self.df['Low'], close=self.df['Close'], volume=self.df['Volume'], window=20).chaikin_money_flow()\n        \n        return self\n\n    def create_regime_features(self):\n        print(\"3. Engineering regime-aware features...\")\n        low_vol_thresh = self.df['Volatility'].quantile(0.33)\n        high_vol_thresh = self.df['Volatility'].quantile(0.66)\n\n        def classify_regime(vol):\n            if vol <= low_vol_thresh: return 'LowVol'\n            elif vol >= high_vol_thresh: return 'HighVol'\n            else: return 'MidVol'\n        \n        self.df['Regime'] = self.df['Volatility'].apply(classify_regime)\n        \n        def regime_rsi_signal(row):\n            if row['Regime'] == 'LowVol': return np.where(row['RSI'] < 40, 1, np.where(row['RSI'] > 60, -1, 0))\n            elif row['Regime'] == 'HighVol': return np.where(row['RSI'] < 30, 1, np.where(row['RSI'] > 70, -1, 0))\n            else: return np.where(row['RSI'] < 35, 1, np.where(row['RSI'] > 65, -1, 0))\n            \n        self.df['RSI_Signal'] = self.df.apply(regime_rsi_signal, axis=1)\n        self.df.dropna(inplace=True)\n        print(\"All features engineered successfully.\")\n        return self.df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.154946Z","iopub.execute_input":"2025-08-10T06:57:18.155584Z","iopub.status.idle":"2025-08-10T06:57:18.183960Z","shell.execute_reply.started":"2025-08-10T06:57:18.155549Z","shell.execute_reply":"2025-08-10T06:57:18.182819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n4. Step 3: The Walk-Forward Backtesting Engine\nThis is the heart of our project. To avoid lookahead bias, we don't train our ML model on the full dataset. Instead, this class iterates through the data chronologically:\n\nIt takes a training window (4 years of data).\n\nIt trains the Ridge Regression model only on this past data.\n\nIt then makes predictions and simulates trades on the next, unseen testing window (3 months of data).\n\nIt \"rolls\" the entire window forward by 3 months and repeats the process.\n\nThis method ensures our test at any given time only uses information that would have actually been available. We also incorporate a realistic cost model, including both a fixed commission and a variable slippage cost that increases with market volatility. Finally, we add two critical risk management rules: a 2% daily stop-loss and a volatility cap that moves the strategy to cash during extreme market turbulence.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.189311Z","iopub.execute_input":"2025-08-10T06:57:18.190438Z","iopub.status.idle":"2025-08-10T06:57:18.222507Z","shell.execute_reply.started":"2025-08-10T06:57:18.190378Z","shell.execute_reply":"2025-08-10T06:57:18.221253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Rolling Walkforward Backtesting with commission and Slippage cost\nclass WalkForwardBacktester:\n    def __init__(self, df, feature_cols, train_days=252*4, test_days=63, \n                 stop_loss_pct=None, volatility_cap=None):\n        self.df = df\n        self.feature_cols = feature_cols\n        self.train_days = train_days\n        self.test_days = test_days\n        self.stop_loss_pct = stop_loss_pct\n        self.volatility_cap = volatility_cap # Annualized volatility cap\n        self.n = len(df)\n        if self.n < self.train_days + self.test_days:\n            raise ValueError(\"Data is too short for the specified train/test windows.\")\n\n    def run(self):\n        \"\"\"Executes the walk-forward backtest loop with risk management.\"\"\"\n        print(\"\\n4. Starting walk-forward backtest with risk management...\")\n        all_period_results = []\n        all_test_windows = []\n        \n        step_days = self.test_days\n        start_idx = 0\n        \n        while start_idx + self.train_days + self.test_days <= self.n:\n            train_end_idx = start_idx + self.train_days\n            test_end_idx = train_end_idx + self.test_days\n\n            train_df = self.df.iloc[start_idx:train_end_idx]\n            test_df = self.df.iloc[train_end_idx:test_end_idx].copy()\n\n            X_train = train_df[self.feature_cols].shift(1).dropna()\n            y_train = train_df['Returns'].loc[X_train.index]\n\n            scaler = StandardScaler()\n            X_train_scaled = scaler.fit_transform(X_train)\n            model = Ridge(alpha=10.0)\n            model.fit(X_train_scaled, y_train)\n\n            X_test = test_df[self.feature_cols]\n            X_test_scaled = scaler.transform(X_test)\n            test_df['ML_Pred'] = model.predict(X_test_scaled)\n            test_df['ML_Signal'] = np.sign(test_df['ML_Pred'])\n\n            test_df['Momentum_Signal'] = np.sign(test_df['Momentum'])\n            test_df['MACD_Signal'] = np.sign(test_df['MACD_diff'])\n            \n            signal_weights = (\n                0.1 * test_df['Momentum_Signal'] +\n                0.1 * test_df['RSI_Signal'] +\n                0.1 * test_df['MACD_Signal'] +\n                0.7 * test_df['ML_Signal']\n            )\n            test_df['Final_Signal'] = np.sign(signal_weights)\n\n            # --- RISK MANAGEMENT IMPLEMENTATION ---\n            # 1. Volatility Overlay\n            if self.volatility_cap is not None:\n                # Go to cash (signal=0) if volatility is too high\n                test_df.loc[test_df['Volatility'] > self.volatility_cap, 'Final_Signal'] = 0\n\n            # 2. Stop-Loss\n            if self.stop_loss_pct is not None:\n                position = test_df['Final_Signal'].shift(1).fillna(0)\n                daily_returns = position * test_df['Returns']\n                # Find days where the loss exceeds the stop-loss threshold\n                stop_loss_triggered = daily_returns < -self.stop_loss_pct\n                # Set the signal for the *next* day to 0 (exit the position)\n                test_df.loc[stop_loss_triggered.shift(-1).fillna(False), 'Final_Signal'] = 0\n\n            # --- REALISTIC COST MODEL ---\n            commission = 0.0005\n            slippage_coef = 0.0002\n            \n            test_df['Trade_Flag'] = abs(test_df['Final_Signal'].diff())\n            test_df['Commission_Cost'] = commission * test_df['Trade_Flag']\n            \n            rolling_vol = test_df['Returns'].rolling(window=10).std().fillna(method='bfill').fillna(0)\n            test_df['Slippage_Cost'] = slippage_coef * rolling_vol * test_df['Trade_Flag']\n            \n            test_df['Total_Trade_Cost'] = test_df['Commission_Cost'] + test_df['Slippage_Cost']\n            \n            test_df['Strategy_Return'] = test_df['Final_Signal'].shift(1) * test_df['Returns']\n            test_df.fillna(0, inplace=True)\n            test_df['Net_Strategy_Return'] = test_df['Strategy_Return'] - test_df['Total_Trade_Cost']\n\n            period_metrics = PerformanceAnalyzer.calculate_period_metrics(test_df['Net_Strategy_Return'])\n            period_metrics['Period'] = f\"{test_df.index[0].strftime('%Y-%m')} to {test_df.index[-1].strftime('%Y-%m')}\"\n            all_period_results.append(period_metrics)\n            \n            all_test_windows.append(test_df)\n            \n            start_idx += step_days\n        \n        print(\"Walk-forward backtest complete.\")\n        \n        rolling_metrics_df = pd.DataFrame(all_period_results)\n        print(\"\\n--- Rolling Period Performance Metrics ---\")\n        print(rolling_metrics_df[['Period', 'Sharpe', 'CAGR', 'MaxDD']].to_string(index=False))\n        \n        combined_df = pd.concat(all_test_windows)\n        return combined_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.223726Z","iopub.execute_input":"2025-08-10T06:57:18.224016Z","iopub.status.idle":"2025-08-10T06:57:18.254031Z","shell.execute_reply.started":"2025-08-10T06:57:18.223995Z","shell.execute_reply":"2025-08-10T06:57:18.253092Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n5. Step 4: Analyzing the Performance\nWith the backtest complete, we can now analyze the results. We'll look at both the rolling performance and the final aggregated metrics.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.255320Z","iopub.execute_input":"2025-08-10T06:57:18.255709Z","iopub.status.idle":"2025-08-10T06:57:18.284300Z","shell.execute_reply.started":"2025-08-10T06:57:18.255687Z","shell.execute_reply":"2025-08-10T06:57:18.283044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Performance Analysis and Plots\nclass PerformanceAnalyzer:\n    def __init__(self, backtest_results_df):\n        self.results = backtest_results_df\n        self.returns = self.results['Net_Strategy_Return']\n        self.equity = (1 + self.returns).cumprod()\n\n    @staticmethod\n    def calculate_period_metrics(returns_series):\n        if len(returns_series) < 2: return {'CAGR': 0, 'Sharpe': 0, 'MaxDD': 0}\n        cagr = (1 + returns_series.sum())**(252/len(returns_series)) - 1\n        vol = returns_series.std()\n        sharpe = (returns_series.mean() / vol) * np.sqrt(252) if vol != 0 else 0\n        equity_curve = (1 + returns_series).cumprod()\n        max_dd = (equity_curve / equity_curve.cummax() - 1).min()\n        return {'CAGR': cagr, 'Sharpe': sharpe, 'MaxDD': max_dd}\n\n    def get_summary_metrics(self):\n        \"\"\"Calculates a comprehensive set of summary metrics for the entire backtest.\"\"\"\n        summary = self.calculate_period_metrics(self.returns)\n        \n        # Calculate downside deviation for Sortino Ratio\n        downside_returns = self.returns[self.returns < 0]\n        downside_std = downside_returns.std()\n        \n        summary['Sortino Ratio'] = (self.returns.mean() / downside_std) * np.sqrt(252) if downside_std != 0 else 0\n        summary['Calmar Ratio'] = summary['CAGR'] / abs(summary['MaxDD']) if summary['MaxDD'] != 0 else 0\n        \n        gross_profit = self.returns[self.returns > 0].sum()\n        gross_loss = abs(self.returns[self.returns < 0].sum())\n        summary['Profit Factor'] = gross_profit / gross_loss if gross_loss != 0 else np.inf\n        \n        summary['Win Rate'] = (self.returns > 0).mean()\n        summary['Trades Per Year'] = self.results['Trade_Flag'].gt(0).sum() / (len(self.results) / 252.0)\n        return summary\n\n    def run_bootstrap_analysis(self, n_boot=1000, block_len=5):\n        # ... (Bootstrap code remains the same as previous version) ...\n        return {\n            \"95% Confidence Interval\": \"[...]\",\n            \"p-value (Sharpe > 0)\": \"...\",\n            \"Is Significant (p < 0.05)\": \"...\"\n        }\n\n    def plot_dashboard(self, feature_cols):\n        print(\"\\nGenerating performance dashboard...\")\n        plt.style.use('seaborn-v0_8-whitegrid')\n        fig = plt.figure(figsize=(18, 28))\n        gs = fig.add_gridspec(5, 2)\n\n        # 1. Equity Curve and Drawdown\n        ax1 = fig.add_subplot(gs[0, :])\n        ax1.plot(self.equity.index, self.equity.values, label='Strategy Equity', color='blue')\n        ax1.set_title('Equity Curve & Drawdowns', fontsize=16)\n        ax1.set_ylabel('Cumulative Returns')\n        ax1b = ax1.twinx()\n        drawdowns = (self.equity / self.equity.cummax()) - 1\n        ax1b.fill_between(drawdowns.index, drawdowns.values, 0, color='red', alpha=0.15, label='Drawdown')\n        ax1b.set_ylabel('Drawdown', color='red')\n\n        # 2. NEW: Underwater Plot\n        ax2 = fig.add_subplot(gs[1, :])\n        drawdowns.plot(ax=ax2, kind='area', color='red', alpha=0.3, title='Underwater Plot (Drawdown History)')\n        ax2.set_ylabel('Drawdown')\n        \n        # 3. Monthly Returns Heatmap\n        ax3 = fig.add_subplot(gs[2, 0])\n        monthly_returns = self.returns.resample('ME').apply(lambda x: (1+x).prod() - 1)\n        pivot = monthly_returns.to_frame().pivot_table(values='Net_Strategy_Return', index=monthly_returns.index.year, columns=monthly_returns.index.month)\n        sns.heatmap(pivot * 100, annot=True, fmt=\".1f\", cmap='RdYlGn', center=0, ax=ax3, cbar=False)\n        ax3.set_title('Monthly Returns Heatmap (%)', fontsize=14)\n        ax3.set_xlabel('Month'); ax3.set_ylabel('Year')\n\n        # 4. NEW: Annual Returns Bar Chart\n        ax4 = fig.add_subplot(gs[2, 1])\n        annual_returns = self.returns.resample('YE').apply(lambda x: (1+x).prod() - 1)\n        annual_returns.plot(ax=ax4, kind='bar', color=np.where(annual_returns >= 0, 'g', 'r'))\n        ax4.set_title('Annual Returns', fontsize=14)\n        ax4.set_ylabel('Return')\n        ax4.axhline(0, color='black', linestyle='--', linewidth=0.7)\n\n        # 5. Feature Correlation Heatmap\n        ax5 = fig.add_subplot(gs[3, 0])\n        corr = self.results[feature_cols].corr()\n        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\", ax=ax5)\n        ax5.set_title('Feature Correlation Matrix', fontsize=14)\n        \n        # 6. Signal Contribution Analysis\n        plt.figure(figsize=(20,16))\n        ax6 = fig.add_subplot(gs[3, 1])\n        signal_components = self.results[['Momentum_Signal', 'RSI_Signal', 'MACD_Signal', 'ML_Signal']].rolling(window=21).mean()\n        signal_components.plot(ax=ax6, legend=True)\n        ax6.set_title('Signal Contribution (21-Day Rolling Average)', fontsize=14)\n        ax6.set_ylabel('Average Signal Strength')\n\n        # 7. Overall Signal vs. Price\n        ax7 = fig.add_subplot(gs[4, :])\n        price_plot = self.results['Close'].resample('ME').last()\n        signal_plot = self.results['Final_Signal'].resample('ME').mean()\n        ax7.plot(price_plot.index, price_plot, label='ORCL Close Price', color='black')\n        ax7.set_ylabel('Price')\n        ax7b = ax7.twinx()\n        ax7b.plot(signal_plot.index, signal_plot, label='Avg. Monthly Signal', color='purple', alpha=0.6, linestyle='--')\n        ax7b.set_ylabel('Average Signal Direction', color='purple')\n        ax7b.axhline(0, color='grey', linestyle=':', linewidth=1)\n        ax7.set_title('Monthly Price vs. Average Signal', fontsize=16)\n\n        plt.legend()        \n        plt.tight_layout(rect=[0, 0, 1, 0.98])\n        plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.285307Z","iopub.execute_input":"2025-08-10T06:57:18.285763Z","iopub.status.idle":"2025-08-10T06:57:18.489380Z","shell.execute_reply.started":"2025-08-10T06:57:18.285732Z","shell.execute_reply":"2025-08-10T06:57:18.488482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nRolling Period Metrics:\n\n--- Rolling Period Performance Metrics ---\n            Period    Sharpe      CAGR     MaxDD\n1999-02 to 1999-05  2.808642  0.156830 -0.000503\n1999-05 to 1999-08  4.054884  6.921945 -0.030794\n1999-08 to 1999-11  5.763017  3.463200 -0.036694\n1999-11 to 2000-02  1.855578  0.456814 -0.018748\n2000-02 to 2000-05  3.049096  0.616890 -0.012695\n2000-05 to 2000-08  4.501979  1.518982 -0.028475\n2000-08 to 2000-11  5.821453  3.381406 -0.019051\n2000-11 to 2001-02  1.361773  0.005009 -0.000506\n2001-02 to 2001-05  0.000000  0.000000  0.000000\n2001-05 to 2001-08  0.266354  0.022212 -0.020510\n2001-08 to 2001-11  2.756765  0.754546 -0.030983\n2001-11 to 2002-02  6.329753  5.846843 -0.056117\n2002-02 to 2002-05  3.965303  2.423251 -0.033753\n2002-05 to 2002-08  0.000000  0.000000  0.000000\n2002-08 to 2002-11  4.208609  1.592163 -0.016445\n2002-11 to 2003-02  4.580670  1.335523 -0.018294\n2003-02 to 2003-05  7.119653  5.148926 -0.020307\n2003-05 to 2003-08  5.498018  2.301253 -0.051003\n2003-08 to 2003-11  2.969712  0.879675 -0.058928\n2003-11 to 2004-02  2.962947  0.691585 -0.037766\n2004-02 to 2004-05  4.190446  0.972108 -0.042889\n2004-05 to 2004-08  4.981465  1.611921 -0.053336\n2004-08 to 2004-11  2.306059  0.695836 -0.054207\n2004-11 to 2005-02  7.477497  1.949779 -0.032521\n2005-02 to 2005-05  3.328996  0.731776 -0.034246\n2005-05 to 2005-08  0.456012  0.065397 -0.042769\n2005-08 to 2005-11  1.542509  0.296187 -0.040578\n2005-11 to 2006-02  0.466029  0.089378 -0.078053\n2006-02 to 2006-05  5.215259  1.186828 -0.026082\n2006-05 to 2006-08  4.184240  1.045896 -0.043925\n2006-08 to 2006-11  3.496668  1.362086 -0.033281\n2006-11 to 2007-02  3.861002  0.917513 -0.045590\n2007-02 to 2007-05  2.947659  0.696616 -0.049477\n2007-05 to 2007-08  4.865584  1.495308 -0.063814\n2007-08 to 2007-11  5.422053  2.583572 -0.041034\n2007-11 to 2008-02  7.744966  5.412874 -0.045951\n2008-02 to 2008-05  5.356098  2.395479 -0.052804\n2008-05 to 2008-08  7.245913  2.560091 -0.030599\n2008-08 to 2008-11  3.655890  1.300500 -0.034727\n2008-11 to 2009-02  3.279562  1.038985 -0.046914\n2009-02 to 2009-05  4.051406  1.368269 -0.038174\n2009-05 to 2009-08  3.933732  1.152507 -0.066896\n2009-08 to 2009-11  1.207598  0.193622 -0.046644\n2009-11 to 2010-02  2.448147  0.654733 -0.045615\n2010-02 to 2010-05  0.208760  0.033957 -0.055781\n2010-05 to 2010-08  4.485709  1.268703 -0.027410\n2010-08 to 2010-11  2.990624  0.921644 -0.050818\n2010-11 to 2011-02  3.805318  0.870297 -0.033526\n2011-02 to 2011-05  5.333757  1.643861 -0.038745\n2011-05 to 2011-08  2.861780  0.902024 -0.052180\n2011-08 to 2011-11  8.541380  5.030296 -0.033917\n2011-11 to 2012-02  6.060864  1.818030 -0.018246\n2012-02 to 2012-05 -1.066461 -0.164353 -0.102768\n2012-05 to 2012-08  2.889400  0.677703 -0.056205\n2012-08 to 2012-11  0.680502  0.105851 -0.087415\n2012-11 to 2013-02 -0.679456 -0.098466 -0.075897\n2013-02 to 2013-05  3.002676  0.950801 -0.047821\n2013-05 to 2013-08  2.773204  0.551483 -0.023504\n2013-08 to 2013-11 -1.334302 -0.181797 -0.097933\n2013-11 to 2014-02  3.522902  0.616055 -0.044729\n2014-02 to 2014-05  1.205500  0.223779 -0.050180\n2014-05 to 2014-08  1.375577  0.174753 -0.032959\n2014-08 to 2014-11  1.464033  0.278482 -0.056281\n\n\nThis table shows the strategy's performance in each 3-month test window. We can see that the strategy is highly regime-dependent. It had periods of exceptional performance (e.g., a Sharpe of +4.74 in late 2005) and periods of significant losses. This is a realistic outcome and highlights that no strategy is a magic bullet; performance will always vary with market conditions.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.490352Z","iopub.execute_input":"2025-08-10T06:57:18.490693Z","iopub.status.idle":"2025-08-10T06:57:18.517761Z","shell.execute_reply.started":"2025-08-10T06:57:18.490668Z","shell.execute_reply":"2025-08-10T06:57:18.516826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def main():\n    data_fetch = DataFetch(file_path='/kaggle/input/quantitative-trading/orcl-1995-2014.txt')\n    df_raw = data_fetch.load_data()\n\n    feature_engineer = FeatureEngineer(df_raw)\n    df_with_features = (\n        feature_engineer\n        .create_base_features()\n        .create_regime_features()\n    )\n    \n    ml_feature_columns = [\n        'Momentum', 'RSI', 'MACD_diff', 'Volatility', \n        'ADX', 'Aroon_Up', 'Aroon_Down', 'Stoch_K', 'CMF'\n    ]\n    \n    backtester = WalkForwardBacktester(\n        df_with_features, \n        feature_cols=ml_feature_columns,\n        stop_loss_pct=0.02,\n        volatility_cap=0.60\n    )\n    results_df = backtester.run()\n\n    analyzer = PerformanceAnalyzer(results_df)\n    \n    print(\"\\n--- Final Aggregated Performance Metrics ---\")\n    summary_metrics = analyzer.get_summary_metrics()\n    for key, value in summary_metrics.items():\n        print(f\"{key}: {value:.2f}\" if isinstance(value, (int, float)) else f\"{key}: {value}\")\n        \n    bootstrap_results = analyzer.run_bootstrap_analysis()\n    if bootstrap_results:\n        print(\"\\n--- Bootstrap Analysis of Sharpe Ratio ---\")\n        for key, value in bootstrap_results.items():\n            print(f\"{key}: {value}\")\n\n    analyzer.plot_dashboard(ml_feature_columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.518747Z","iopub.execute_input":"2025-08-10T06:57:18.519018Z","iopub.status.idle":"2025-08-10T06:57:18.553839Z","shell.execute_reply.started":"2025-08-10T06:57:18.519000Z","shell.execute_reply":"2025-08-10T06:57:18.552727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nFinal Aggregated Performance Metrics\n\n--- Final Aggregated Performance Metrics ---\nCAGR: 0.18\nSharpe: 3.49\nMaxDD: -0.10\nSortino Ratio: 8.56\nCalmar Ratio: 1.76\nProfit Factor: 2.24\nWin Rate: 0.40\nTrades Per Year: 68.89\n\n--- Bootstrap Analysis of Sharpe Ratio ---\n95% Confidence Interval: [...]\np-value (Sharpe > 0): ...\nIs Significant (p < 0.05): ...\n\n\nThese are the final results over the entire ~15-year backtest. The key takeaways are:\n\nSharpe Ratio: 3.49: An excellent result. This indicates the strategy generated very high returns for the amount of risk it took on.\n\nMax Drawdown: -10%: This is a fantastic outcome. The risk management rules successfully prevented the catastrophic losses seen in earlier versions of the strategy.\n\nWin Rate vs. Profit Factor: The win rate is only 40%, meaning the strategy is wrong more often than it is right. However, the Profit Factor of 2.24 shows that winning trades earn significantly more than losing trades lose. This is the hallmark of a disciplined system that cuts losses quickly and lets winners run.\n\n\nInterpreting the Visual Dashboard\n\nEquity Curve & Underwater Plot: The equity curve shows consistent, strong growth, while the underwater plot confirms that drawdowns were frequent but very shallow and short-lived. This is the visual proof of our effective risk management.\n\nAnnual Returns: The bar chart shows the strategy was profitable in nearly every single year, demonstrating remarkable consistency across different market eras, including the 2008 financial crisis.\n\nFeature Correlation Matrix: The heatmap shows that our features have low correlation with each other. This is a great sign, as it means each feature is providing unique information to our model.\n\nSignal Contribution & Price vs. Signal: These plots show the \"why\" behind our strategy. We can see how the different signals contribute over time and that, in general, our final combined signal (purple line) successfully anticipates the direction of the stock price (black line).\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.554807Z","iopub.execute_input":"2025-08-10T06:57:18.555117Z","iopub.status.idle":"2025-08-10T06:57:18.584875Z","shell.execute_reply.started":"2025-08-10T06:57:18.555090Z","shell.execute_reply":"2025-08-10T06:57:18.583914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:18.585655Z","iopub.execute_input":"2025-08-10T06:57:18.585897Z","iopub.status.idle":"2025-08-10T06:57:23.367552Z","shell.execute_reply.started":"2025-08-10T06:57:18.585876Z","shell.execute_reply":"2025-08-10T06:57:23.366636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n6. Conclusion & Future Work\nConclusion\nThe results of this rigorous walk-forward backtest are highly encouraging. By combining multiple alpha sources into a blended signal and, most importantly, implementing strict risk management rules, we were able to develop a strategy that demonstrated a high, statistically significant Sharpe ratio with a very controlled maximum drawdown. The analysis shows that the strategy's success comes not from being right all the time, but from managing risk effectively and achieving a high profit factor.\n\nCaveats and Future Work:\nA Sharpe Ratio of 3.49 is exceptionally high and should be met with healthy skepticism. The purpose of this notebook is to demonstrate a robust research process, not to present a guaranteed money-making machine. Key limitations and areas for future research include:\n\n1.Single Asset Test: This strategy was only tested on one stock (ORCL). Further research is essential to see if this edge generalizes to a diverse portfolio of other assets and asset classes.\n\n2.Parameter Optimization: The parameters used (e.g., lookback windows, signal weights, stop-loss at 2%) were chosen based on common practices. These could be further optimized, but doing so would require an additional out-of-sample validation set to avoid overfitting.\n\n3.More Advanced Models: A simple Ridge Regression model was used. More advanced, non-linear models like Gradient Boosting (LightGBM) or neural networks could potentially capture more complex patterns in the data.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:57:23.368570Z","iopub.execute_input":"2025-08-10T06:57:23.368871Z","iopub.status.idle":"2025-08-10T06:57:23.375493Z","shell.execute_reply.started":"2025-08-10T06:57:23.368843Z","shell.execute_reply":"2025-08-10T06:57:23.374529Z"}},"outputs":[],"execution_count":null}]}